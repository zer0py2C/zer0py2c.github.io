	<!DOCTYPE HTML>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Flink与PyFlink流处理（二） | zer0py2c个人博客</title>
  <meta name="author" content="zer0py2c">
  
  <meta name="description" content="一、Yarn集群部署PyFlink作业1.多种作业部署方式介绍
local

12# SingleJVM：启动一个minicluster，作业会提交到minicluster中执行python3 pyflink_demo.py


standalone

123# SingleNode：启动本地fli">
  
  
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  <meta property="og:title" content="Flink与PyFlink流处理（二）"/>
  <meta property="og:site_name" content="zer0py2c个人博客"/>

  
  
		<!-- favicon -->
		<link rel="apple-touch-icon" sizes="57x57" href="/apple-touch-icon-57x57.png">
		<link rel="apple-touch-icon" sizes="60x60" href="/apple-touch-icon-60x60.png">
		<link rel="apple-touch-icon" sizes="72x72" href="/apple-touch-icon-72x72.png">
		<link rel="apple-touch-icon" sizes="76x76" href="/apple-touch-icon-76x76.png">
		<link rel="apple-touch-icon" sizes="114x114" href="/apple-touch-icon-114x114.png">
		<link rel="apple-touch-icon" sizes="120x120" href="/apple-touch-icon-120x120.png">
		<link rel="apple-touch-icon" sizes="144x144" href="/apple-touch-icon-144x144.png">
		<link rel="apple-touch-icon" sizes="152x152" href="/apple-touch-icon-152x152.png">
		<link rel="icon" type="image/png" href="/favicon-32x32.png" sizes="32x32">
		<link rel="icon" type="image/png" href="/favicon-96x96.png" sizes="96x96">
		<link rel="icon" type="image/png" href="/favicon-16x16.png" sizes="16x16">
		<link rel="manifest" href="/manifest.json">
		<meta name="msapplication-TileColor" content="#009688">
		<meta name="msapplication-TileImage" content="/mstile-144x144.png">
		<meta name="theme-color" content="#009688">
		<!-- favicon end -->
    <!-- <link href="/favicon.ico" rel="icon"> -->
  

  <!-- toc -->
  <link rel="stylesheet" href="/libs/tocify/jquery.tocify.css" media="screen" type="text/css">

  <!-- <link rel="stylesheet" href="/libs/bs/css/bootstrap.min.css" media="screen" type="text/css"> -->
  <link rel="stylesheet" href="//apps.bdimg.com/libs/bootstrap/3.3.4/css/bootstrap.min.css" media="screen" type="text/css">

  <!-- material design -->
	<!-- <link rel="stylesheet" href="/libs/bs-material/css/ripples.min.css" media="screen" type="text/css"> -->
  <link rel="stylesheet" href="//apps.bdimg.com/libs/bootstrap-material/0.3.0/css/ripples.min.css" media="screen" type="text/css">
  <!-- <link rel="stylesheet" href="/libs/bs-material/css/material.min.css" media="screen" type="text/css"> -->
	<link rel="stylesheet" href="//apps.bdimg.com/libs/bootstrap-material/0.3.0/css/material.min.css" media="screen" type="text/css">

  <link rel="stylesheet" href="/css/highlight.light.css" media="screen" type="text/css">

  <link rel="stylesheet" href="/css/style.css" media="screen" type="text/css">

  

  

  <script src="//apps.bdimg.com/libs/jquery/2.0.3/jquery.min.js"></script>
	<script>window.jQuery || document.write('<script src="/libs/jquery-2.0.3.min.js" type="text/javascript"><\/script>')</script>

<meta name="generator" content="Hexo 6.3.0"></head>

 	<body>
	  <nav class="navbar navbar-default">
    <div class="container">
        <div class="navbar-header">
            <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
            <span class="sr-only">菜单</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="/">zer0py2c个人博客</a>
        </div>
        <div id="navbar" class="collapse navbar-collapse">
            <ul class="nav navbar-nav navbar-right">
                
                <li>
                    <a href="/" title="">
                    <i class="fa fa-home"></i>首页
                    </a>
                </li>
                
                <li>
                    <a href="/archives" title="">
                    <i class="fa fa-list"></i>存档
                    </a>
                </li>
                
                <li>
                    <a href="/about" title="关于作者">
                    <i class="fa fa-info-circle"></i>关于
                    </a>
                </li>
                
                <li>
                    <a href="/atom.xml" title="订阅源">
                    <i class="fa fa-rss"></i>RSS
                    </a>
                </li>
                
            </ul>
        </div>
    </div>
</nav>

	  <div class="container" >
	    <div class="row">
	
	<div class="col-md-9 center-content">
	

		<div class="content">
			<!-- index -->
		   

			  		<h2>Flink与PyFlink流处理（二）</h2>
					
					<div>
						<span class="post-time">2023-09-04 17:06:37</span>
					</div>	
					

					<div class="article-content">
						<h3 id="一、Yarn集群部署PyFlink作业"><a href="#一、Yarn集群部署PyFlink作业" class="headerlink" title="一、Yarn集群部署PyFlink作业"></a>一、Yarn集群部署PyFlink作业</h3><h5 id="1-多种作业部署方式介绍"><a href="#1-多种作业部署方式介绍" class="headerlink" title="1.多种作业部署方式介绍"></a>1.多种作业部署方式介绍</h5><ul>
<li>local</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">SingleJVM：启动一个minicluster，作业会提交到minicluster中执行</span></span><br><span class="line">python3 pyflink_demo.py</span><br></pre></td></tr></table></figure>

<ul>
<li>standalone</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">SingleNode：启动本地flink，作业会提交到standalone集群中执行</span></span><br><span class="line"><span class="meta prompt_">$</span><span class="language-bash">FLINK_HOME/bin/start-cluster.sh <span class="built_in">local</span></span></span><br><span class="line">flink run -m localhost:8081 -py pyflink_demo.py</span><br></pre></td></tr></table></figure>

<ul>
<li>yarn per job</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">提交作业时会为每个作业单独启动一个Flink集群</span></span><br><span class="line">flink run -m yarn-cluster -yqu bigTask -py pyflink_demo.py</span><br></pre></td></tr></table></figure>

<ul>
<li>yarn session</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">先在Yarn上启动一个Flink集群，之后作业都会提交到该Flink集群</span></span><br><span class="line">./yarn-session.sh</span><br><span class="line">flink run -yqu bigTask -py pyflink_demo.py</span><br></pre></td></tr></table></figure>



<h5 id="2-配置Yarn-Flink集群环境"><a href="#2-配置Yarn-Flink集群环境" class="headerlink" title="2.配置Yarn+Flink集群环境"></a>2.配置Yarn+Flink集群环境</h5><p>略</p>
<h5 id="3-配置Python环境"><a href="#3-配置Python环境" class="headerlink" title="3.配置Python环境"></a>3.配置Python环境</h5><ul>
<li>安装依赖</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip3 install apache-flink==1.12.0</span><br></pre></td></tr></table></figure>

<ul>
<li>更新Flink配置项</li>
</ul>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 指定提交PyFlink作业的Python解释器的路径，用于在&quot;flink run&quot;提交Python作业时启动Python进程。</span></span><br><span class="line"><span class="attr">python.client.executable:</span> <span class="string">/usr/local/python3/bin/python3.7</span></span><br><span class="line"><span class="comment"># 指定执行Python UDF的Python解释器的路径。Python UDF执行环境要求Python 3.5+, Apache Beam (version == 2.23.0), Pip (version &gt;= 7.1.0) and SetupTools (version &gt;= 37.0.0)</span></span><br><span class="line"><span class="attr">python.executable:</span> <span class="string">/usr/local/python3/bin/python3.7</span></span><br></pre></td></tr></table></figure>



<h5 id="4-确定线上部署方式"><a href="#4-确定线上部署方式" class="headerlink" title="4.确定线上部署方式"></a>4.确定线上部署方式</h5><ul>
<li>Session模式：所有作业在一个Flink Session中管理，作业间资源隔离较差。</li>
<li>Per Job模式：作业间资源隔离充分，适合大作业线上部署。</li>
</ul>
<h3 id="二、PyFlink-Table-SQL-API"><a href="#二、PyFlink-Table-SQL-API" class="headerlink" title="二、PyFlink Table&#x2F;SQL API"></a>二、PyFlink Table&#x2F;SQL API</h3><p>Flink数据处理的流向：Source -&gt; Transformation -&gt; Sink</p>
<h5 id="1-执行上下文"><a href="#1-执行上下文" class="headerlink" title="1.执行上下文"></a>1.执行上下文</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1. 流处理</span></span><br><span class="line"><span class="comment"># option 1</span></span><br><span class="line">env = StreamExecutionEnvironment.get_execution_environment()</span><br><span class="line">t_env = StreamTableEnvironment.create(env)</span><br><span class="line"><span class="comment"># option 2</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;The param &#x27;execution_environment&#x27; and &#x27;environment_settings&#x27; cannot be used at the same time&quot;&quot;&quot;</span></span><br><span class="line">env_settings = EnvironmentSettings.new_instance().in_streaming_mode().use_blink_planner().build()</span><br><span class="line">t_env = StreamTableEnvironment.create(environment_settings=env_settings)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. 批处理</span></span><br><span class="line"><span class="comment"># option 1</span></span><br><span class="line">env = ExecutionEnvironment.get_execution_environment()</span><br><span class="line">t_env = BatchTableEnvironment.create(env)</span><br><span class="line"><span class="comment"># option 2</span></span><br><span class="line">env_settings = EnvironmentSettings.new_instance().in_streaming_mode().use_blink_planner().build()</span><br><span class="line">t_env = BatchTableEnvironment.create(environment_settings=env_settings)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 配置项</span></span><br><span class="line">t_env.get_config().get_configuration().set_string(<span class="string">&quot;parallelism.default&quot;</span>, <span class="number">1</span>) <span class="comment"># 设置当前作业中算子并行度</span></span><br><span class="line">t_env.get_config().get_configuration().set_string(<span class="string">&quot;pipeline.jars&quot;</span>, <span class="string">&quot;file:///jar_path/xxx.jar&quot;</span>) <span class="comment"># 使用第三方组件，需要提前引入与Flink版本一致的依赖</span></span><br></pre></td></tr></table></figure>



<h5 id="2-算子"><a href="#2-算子" class="headerlink" title="2.算子"></a>2.算子</h5><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Table API: select, alias, filter, where, group<span class="emphasis">_by, distinct, join, join_</span>lateral, order<span class="emphasis">_by, window, to_</span>pandas, execute<span class="emphasis">_insert</span></span><br></pre></td></tr></table></figure>

<p>对于一条完整的SQL语句，可以将其以字符串的形式传递给<code>t_env.sql_query(...)</code>方法，该方法调用后将返回一个Table表。</p>
<h5 id="3-Source-Sink"><a href="#3-Source-Sink" class="headerlink" title="3.Source&amp;Sink"></a>3.Source&amp;Sink</h5><p>Table中Source表和Sink表具有相同的创建语义</p>
<ul>
<li>使用DDL</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># FileSystem Source/Sink</span></span><br><span class="line">t_env.sql_update(</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    create table SourceOrSink (</span></span><br><span class="line"><span class="string">        word VARCHAR,</span></span><br><span class="line"><span class="string">        `count` BIGINT</span></span><br><span class="line"><span class="string">    ) with (</span></span><br><span class="line"><span class="string">        &#x27;connector.type&#x27; = &#x27;filesystem&#x27;,</span></span><br><span class="line"><span class="string">        &#x27;format.type&#x27; = &#x27;csv&#x27;,</span></span><br><span class="line"><span class="string">        &#x27;connector.path&#x27; = &#x27;/tmp/input_or_output&#x27;</span></span><br><span class="line"><span class="string">    )</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Kafka Source/Sink</span></span><br><span class="line"><span class="comment"># platform sample download logs: &quot;username, sample_hash, api_or_web, download_time&quot;</span></span><br><span class="line">t_env.sql_update(</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">	create table SourceOrSink (</span></span><br><span class="line"><span class="string">		username varchar,</span></span><br><span class="line"><span class="string">		sample_hash varchar,</span></span><br><span class="line"><span class="string">		api_or_web tinyint,</span></span><br><span class="line"><span class="string">		download_time timestamp(3)</span></span><br><span class="line"><span class="string">	) with (</span></span><br><span class="line"><span class="string">		&#x27;connector.type&#x27; = &#x27;kafka&#x27;,</span></span><br><span class="line"><span class="string">		&#x27;connector.version&#x27; = &#x27;universal&#x27;, # 不必指定具体的版本号</span></span><br><span class="line"><span class="string">		&#x27;connector.topic&#x27; = &#x27;platform_download_log&#x27;,</span></span><br><span class="line"><span class="string">		&#x27;connector.properties.zookeeper.connect&#x27; = &#x27;localhost:2181&#x27;,</span></span><br><span class="line"><span class="string">		&#x27;connector.properties.bootstrap.servers&#x27; = &#x27;localhost:9092&#x27;,</span></span><br><span class="line"><span class="string">		&#x27;connector.properties.scan.startup.mode&#x27; = &#x27;latest-offset&#x27;,</span></span><br><span class="line"><span class="string">		&#x27;format.type&#x27; = &#x27;csv&#x27;,</span></span><br><span class="line"><span class="string">		&#x27;format.ignore-parse-errors&#x27; = &#x27;true&#x27; # 解析异常时，跳过当前字段数据（字段将置为null）</span></span><br><span class="line"><span class="string">	)</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<ul>
<li>使用TableDescriptor</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># FileSystem Source/Sink</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">with_format: 告诉Flink如何处理源数据中的每个元素</span></span><br><span class="line"><span class="string">with_schema: 定义表的结构</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line">t_env.connect(</span><br><span class="line">    FileSystem().path(<span class="string">&quot;file:///tmp/test_file&quot;</span>)</span><br><span class="line">).with_format(</span><br><span class="line">    OldCsv()</span><br><span class="line">        .field_delimiter(<span class="string">&quot;,&quot;</span>)</span><br><span class="line">    	.line_delimiter(<span class="string">&quot;\\n&quot;</span>)</span><br><span class="line">        .field(<span class="string">&quot;a&quot;</span>, DataTypes.INT())</span><br><span class="line">        .field(<span class="string">&quot;b&quot;</span>, DataTypes.STRING())</span><br><span class="line">).with_schema(</span><br><span class="line">    Schema()</span><br><span class="line">        .field(<span class="string">&quot;a&quot;</span>, DataTypes.INT())</span><br><span class="line">        .field(<span class="string">&quot;b&quot;</span>, DataTypes.STRING())</span><br><span class="line">).create_temporary_table(<span class="string">&quot;source_or_sink&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Kafka Source/Sink</span></span><br><span class="line"><span class="comment"># platform sample download logs: &quot;username, sample_hash, api_or_web, download_time&quot;</span></span><br><span class="line">t_env.connect(</span><br><span class="line">    Kafka()</span><br><span class="line">        .version(<span class="string">&quot;universal&quot;</span>)</span><br><span class="line">        .topic(<span class="string">&quot;platform_download_log&quot;</span>)</span><br><span class="line">        .<span class="built_in">property</span>(<span class="string">&quot;zookeeper.connect&quot;</span>, <span class="string">&quot;localhost:2181&quot;</span>)</span><br><span class="line">        .<span class="built_in">property</span>(<span class="string">&quot;bootstrap.servers&quot;</span>, <span class="string">&quot;localhost:9092&quot;</span>)</span><br><span class="line">    	.start_from_latest()</span><br><span class="line">).with_format(</span><br><span class="line">    Json()</span><br><span class="line">        .fail_on_missing_field(<span class="literal">False</span>) <span class="comment"># False: 字段丢失时，置为null</span></span><br><span class="line">        .schema(DataTypes.ROW([</span><br><span class="line">            DataTypes.FIELD(<span class="string">&quot;username&quot;</span>, DataTypes.STRING()),</span><br><span class="line">            DataTypes.FIELD(<span class="string">&quot;sample_hash&quot;</span>, DataTypes.STRING()),</span><br><span class="line">            DataTypes.FIELD(<span class="string">&quot;api_or_web&quot;</span>, DataTypes.TINYINT()),</span><br><span class="line">            DataTypes.FIELD(<span class="string">&quot;download_time&quot;</span>, DataTypes.TIMESTAMP(<span class="number">3</span>)),</span><br><span class="line">        ]))</span><br><span class="line">).with_schema(</span><br><span class="line">    Schema()</span><br><span class="line">        .field(<span class="string">&quot;username&quot;</span>, DataTypes.STRING())</span><br><span class="line">        .field(<span class="string">&quot;sample_hash&quot;</span>, DataTypes.STRING())</span><br><span class="line">        .field(<span class="string">&quot;api_or_web&quot;</span>, DataTypes.TINYINT())</span><br><span class="line">        .field(<span class="string">&quot;download_time&quot;</span>, DataTypes.TIMESTAMP(<span class="number">3</span>))</span><br><span class="line">).create_temporary_table(<span class="string">&quot;source_or_sink&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Elasticsearch Source/Sink</span></span><br><span class="line">t_env.connect(</span><br><span class="line">    Elasticsearch()</span><br><span class="line">        .version(<span class="string">&quot;7&quot;</span>)</span><br><span class="line">        .host(<span class="string">&quot;localhost&quot;</span>, <span class="number">9200</span>, <span class="string">&quot;http&quot;</span>)</span><br><span class="line">        .index(<span class="string">&quot;test_sample_hash&quot;</span>)</span><br><span class="line">        .document_type(<span class="string">&quot;_doc&quot;</span>)</span><br><span class="line">).with_format(</span><br><span class="line">    ...</span><br><span class="line">).with_schema(</span><br><span class="line">    ...</span><br><span class="line">).create_temporary_table(<span class="string">&quot;source_or_sink&quot;</span>)</span><br></pre></td></tr></table></figure>



<h5 id="4-案例回顾"><a href="#4-案例回顾" class="headerlink" title="4.案例回顾"></a>4.案例回顾</h5><p>CDN日志的解析一般有一个通用的架构模式，就是首先要将各个边缘节点的日志数据进行采集，一般会采集到消息队列，然后将消息队列和实时计算集群集成进行实时的日志分析，最后将分析的结果写到存储系统里面。将架构实例化，消息队列采用Kafka，实时计算采用Flink，最终将数据存储到MySQL中。如下图所示：</p>
<p><img src="/images/15852891954996.jpg" alt="15852891954996"></p>
<figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CDN日志假数据格式 &quot;uuid,client<span class="emphasis">_ip,request_</span>time,response<span class="emphasis">_size,uri&quot;</span></span><br></pre></td></tr></table></figure>

<p>本案例将从CDN访问日志中，获取如下统计指标：</p>
<ul>
<li>按照地区统计资源访问量</li>
<li>按照地区统计资源下载量</li>
<li>按照地区统计资源平均下载速度</li>
</ul>
<p>由于地区信息无法直接从访问日志中直接提取，Flink中也没有现成的transformation API，所以我们需要自定义实现一个逻辑函数。我们可以借助地理区域查询服务，根据IP地址获取地理位置信息。</p>
<h5 id="5-用户自定义函数"><a href="#5-用户自定义函数" class="headerlink" title="5.用户自定义函数"></a>5.用户自定义函数</h5><ul>
<li>Scalar Function(UDF)</li>
</ul>
<p>Scalar Function将0、1或者多个值作为输入参数，最后返回一个值。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 方法1：继承ScalarFunction，并实现eval方法</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Add</span>(<span class="title class_ inherited__">ScalarFunction</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">eval</span>(<span class="params">self, i, j</span>):</span><br><span class="line">        <span class="keyword">return</span> i + j</span><br><span class="line"></span><br><span class="line">add = udf(Add(), result_type=DataTypes.BIGINT())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 方法2：decorator function/装饰器</span></span><br><span class="line"><span class="meta">@udf(<span class="params">result_type=DataTypes.BIGINT(<span class="params"></span>)</span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">add</span>(<span class="params">i, j</span>):</span><br><span class="line">    <span class="keyword">return</span> i + j</span><br><span class="line"></span><br><span class="line"><span class="comment"># 方法3：lambda function/lambda表达式</span></span><br><span class="line">add = udf(<span class="keyword">lambda</span> i, j: i + j, result_type=DataTypes.BIGINT())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 方法4：callable function</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">CallableAdd</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__call__</span>(<span class="params">self, i, j</span>):</span><br><span class="line">        <span class="keyword">return</span> i + j</span><br><span class="line"></span><br><span class="line">add = udf(CallableAdd(), result_type=DataTypes.BIGINT())</span><br></pre></td></tr></table></figure>

<ul>
<li>Table Function(UDTF)</li>
</ul>
<p>与Scalar Function类似，Table Function将0、1或者多个值作为输入参数。不同的是，后者能返回任意数量的行作为输出，而不是一个值，而且一行输出数据可以包含多个列。其返回类型可以是可迭代对象、迭代器或生成器。</p>
<p>演示：use_udf.py</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 方法1：继承TableFunction，并实现eval方法</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Split</span>(<span class="title class_ inherited__">TableFunction</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">eval</span>(<span class="params">self, string</span>):</span><br><span class="line">        <span class="keyword">for</span> s <span class="keyword">in</span> string.split(<span class="string">&quot; &quot;</span>):</span><br><span class="line">            <span class="keyword">yield</span> s, <span class="built_in">len</span>(s)</span><br><span class="line"></span><br><span class="line">split = udtf(</span><br><span class="line">    Split(),</span><br><span class="line">    input_types=DataTypes.STRING(),</span><br><span class="line">    result_types=[DataTypes.STRING(), DataTypes.INT()]</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 方法2：decorator function/装饰器</span></span><br><span class="line"><span class="meta">@udtf(<span class="params">result_types=DataTypes.BIGINT(<span class="params"></span>)</span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">generator_func</span>(<span class="params">x</span>):</span><br><span class="line">      <span class="keyword">yield</span> <span class="number">1</span></span><br><span class="line">      <span class="keyword">yield</span> <span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="meta">@udtf(<span class="params">result_types=DataTypes.BIGINT(<span class="params"></span>)</span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">iterator_func</span>(<span class="params">x</span>):</span><br><span class="line">      <span class="keyword">return</span> <span class="built_in">range</span>(<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">@udtf(<span class="params">result_types=DataTypes.BIGINT(<span class="params"></span>)</span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">iterable_func</span>(<span class="params">x</span>):</span><br><span class="line">      result = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]</span><br><span class="line">      <span class="keyword">return</span> result</span><br></pre></td></tr></table></figure>

<ul>
<li>Aggregate Function(UDAF)</li>
</ul>
<p>Aggregate Function用来针对一组数据进行自定义的聚合计算，每组将产生一条输出数据。</p>
<p>演示：use_udaf.py</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 方法：继承AggregateFunction，并实现如下方法</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">	create_accumulator() 创建并初始化一个累加器</span></span><br><span class="line"><span class="string">	accumulate(...) 聚合逻辑实现，并更新累加器</span></span><br><span class="line"><span class="string">	get_value(...) 获取聚合结果值</span></span><br><span class="line"><span class="string">	retract(...) 从累加器中收回输入值</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">WeightAvg</span>(<span class="title class_ inherited__">AggregateFunction</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">create_accumulator</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="comment"># Row(sum, count)</span></span><br><span class="line">        <span class="keyword">return</span> Row(<span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_value</span>(<span class="params">self, accumulator</span>):</span><br><span class="line">        <span class="keyword">if</span> accumulator[<span class="number">1</span>] == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> accumulator[<span class="number">0</span>] / accumulator[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">accumulate</span>(<span class="params">self, accumulator, value, weight</span>):</span><br><span class="line">        accumulator[<span class="number">0</span>] += value * weight</span><br><span class="line">        accumulator[<span class="number">1</span>] += weight</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">retract</span>(<span class="params">self, accumulator, value, weight</span>):</span><br><span class="line">        accumulator[<span class="number">0</span>] -= value * weight</span><br><span class="line">        accumulator[<span class="number">1</span>] -= weight</span><br><span class="line"></span><br><span class="line">weighted_avg = udaf(WeightAvg(),</span><br><span class="line">                    result_type=DataTypes.BIGINT(),</span><br><span class="line">                    accumulator_type=DataTypes.ROW([</span><br><span class="line">                        DataTypes.FIELD(<span class="string">&quot;f0&quot;</span>, DataTypes.BIGINT()),</span><br><span class="line">                        DataTypes.FIELD(<span class="string">&quot;f1&quot;</span>, DataTypes.BIGINT()),</span><br><span class="line">                    ]))</span><br></pre></td></tr></table></figure>



<h5 id="6-案例实现"><a href="#6-案例实现" class="headerlink" title="6.案例实现"></a>6.案例实现</h5><p>演示：cdn_demo.py</p>
<ul>
<li>实现UDF</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@udf(<span class="params">input_types=[DataTypes.STRING(<span class="params"></span>)], result_type=DataTypes.STRING(<span class="params"></span>)</span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">ip_to_province</span>(<span class="params">ip</span>):</span><br><span class="line">   <span class="keyword">try</span>:</span><br><span class="line">       urlobj = urlopen( \</span><br><span class="line">        <span class="string">&#x27;http://whois.pconline.com.cn/ipJson.jsp?ip=%s&#x27;</span> % quote_plus(ip))</span><br><span class="line">       data = <span class="built_in">str</span>(urlobj.read(), <span class="string">&quot;gbk&quot;</span>)</span><br><span class="line">       pos = re.search(<span class="string">&quot;&#123;[^&#123;&#125;]+\&#125;&quot;</span>, data).span()</span><br><span class="line">       geo_data = json.loads(data[pos[<span class="number">0</span>]:pos[<span class="number">1</span>]])</span><br><span class="line">       <span class="keyword">if</span> geo_data[<span class="string">&#x27;pro&#x27;</span>]:</span><br><span class="line">           <span class="keyword">return</span> geo_data[<span class="string">&#x27;pro&#x27;</span>]</span><br><span class="line">       <span class="keyword">else</span>:</span><br><span class="line">           <span class="keyword">return</span> geo_data[<span class="string">&#x27;err&#x27;</span>]</span><br><span class="line">   <span class="keyword">except</span>:</span><br><span class="line">       <span class="keyword">return</span> <span class="string">&quot;UnKnow&quot;</span></span><br></pre></td></tr></table></figure>

<ul>
<li>应用UDF</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">t_env.from_path(<span class="string">&quot;cdn_access_log&quot;</span>)\</span><br><span class="line">   .select(<span class="string">&quot;uuid, &quot;</span></span><br><span class="line">           <span class="string">&quot;ip_to_province(client_ip) as province, &quot;</span> <span class="comment"># IP转换为地区名称</span></span><br><span class="line">           <span class="string">&quot;response_size, request_time&quot;</span>)\</span><br><span class="line">   .group_by(<span class="string">&quot;province&quot;</span>)\</span><br><span class="line">   .select( <span class="comment"># 计算访问量</span></span><br><span class="line">           <span class="string">&quot;province, count(uuid) as access_count, &quot;</span> </span><br><span class="line">           <span class="comment"># 计算下载总量 </span></span><br><span class="line">           <span class="string">&quot;sum(response_size) as total_download,  &quot;</span> </span><br><span class="line">           <span class="comment"># 计算下载速度</span></span><br><span class="line">           <span class="string">&quot;sum(response_size) * 1.0 / sum(request_time) as download_speed&quot;</span>) \</span><br><span class="line">   .insert_into(<span class="string">&quot;cdn_access_statistic&quot;</span>)</span><br></pre></td></tr></table></figure>



<h5 id="7-SQL优化"><a href="#7-SQL优化" class="headerlink" title="7.SQL优化"></a>7.SQL优化</h5><ul>
<li>启用MiniBatch聚合</li>
</ul>
<p>默认情况下，无界聚合算子按如下步骤处理记录：</p>
<blockquote>
<p>（1）读取状态数据</p>
<p>（2）累加&#x2F;撤回记录进行运算</p>
<p>（3）状态写回</p>
<p>（4）同上，处理下一条记录</p>
</blockquote>
<p>这种处理方式会增加对statebackend的压力。MiniBatch聚合原理是缓存一定的记录后再触发聚合，以减少对state的访问次数，从而提升计算吞吐量。通常对于聚合的场景，MiniBatch可以提升系统性能，建议开启。</p>
<p>默认情况下，MiniBatch模式是被禁用的。如果要启用，请在作业中实例化TableEnvironment后插入如下代码。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">t_env = ...</span><br><span class="line"><span class="comment"># access flink configuration</span></span><br><span class="line">configuration = t_env.get_config().get_configuration()</span><br><span class="line"><span class="comment"># set low-level key-value options</span></span><br><span class="line">configuration.set_string(<span class="string">&quot;table.exec.mini-batch.enabled&quot;</span>, <span class="string">&quot;true&quot;</span>) <span class="comment"># 启用MiniBatch模式</span></span><br><span class="line">configuration.set_string(<span class="string">&quot;table.exec.mini-batch.allow-latency&quot;</span>, <span class="string">&quot;5 s&quot;</span>) <span class="comment"># 每5秒缓冲输入记录</span></span><br><span class="line">configuration.set_string(<span class="string">&quot;table.exec.mini-batch.size&quot;</span>, <span class="string">&quot;5000&quot;</span>) <span class="comment"># 每个聚合算子缓冲的记录上限</span></span><br></pre></td></tr></table></figure>

<ul>
<li>启用Local-Global聚合</li>
</ul>
<p>聚合业务场景下，数据流中的记录可能会产生倾斜，即分组产生出了“热点数据”大量地流向下游的某个（SUM、COUNT、MAX、MIN、AVG等）聚合算子实例。Local-Global原理是先在上游算子实例中进行本地聚合，输出本地聚合结果给下游算子，然后在下游算子进行全局聚合。</p>
<p>要启用该机制，请在作业中实例化TableEnvironment后插入如下代码。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">t_env = ...</span><br><span class="line"><span class="comment"># access flink configuration</span></span><br><span class="line">configuration = t_env.get_config().get_configuration()</span><br><span class="line"><span class="comment"># set low-level key-value options</span></span><br><span class="line">configuration.set_string(<span class="string">&quot;table.optimizer.agg-phase-strategy&quot;</span>, <span class="string">&quot;TWO_PHASE&quot;</span>) <span class="comment"># 强制使用具有本地聚合与全局聚合的两阶段聚合模式</span></span><br></pre></td></tr></table></figure>

<ul>
<li>慎用正则函数</li>
</ul>
<p>正则表达式（REGEXP、REGEXP_REPLACE）是非常耗时的操作，对比加减乘除通常有百倍的性能开销，而且正则表达式在某些极端情况下可能会陷入无限循环，导致作业阻塞。建议充分利用LIKE。</p>
<h3 id="三、PyFlink-DataStream-API"><a href="#三、PyFlink-DataStream-API" class="headerlink" title="三、PyFlink DataStream API"></a>三、PyFlink DataStream API</h3><h5 id="1-执行上下文-1"><a href="#1-执行上下文-1" class="headerlink" title="1.执行上下文"></a>1.执行上下文</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 获取流处理执行上下文</span></span><br><span class="line"><span class="keyword">from</span> pyflink.datastream.stream_execution_environment <span class="keyword">import</span> StreamExecutionEnvironment</span><br><span class="line"></span><br><span class="line">env = StreamExecutionEnvironment.get_execution_environment()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取批处理执行上下文</span></span><br><span class="line"><span class="keyword">from</span> pyflink.dataset.execution_environment <span class="keyword">import</span> ExecutionEnvironment</span><br><span class="line"></span><br><span class="line">env = ExecutionEnvironment.get_execution_environment()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 作业配置项</span></span><br><span class="line">env.set_python_executable(...) <span class="comment"># 设置Python解释器的路径</span></span><br><span class="line">env.set_parallelism(...) <span class="comment"># 设置本作业中相关算子的并行度</span></span><br><span class="line">env.set_stream_time_characteristic(...) <span class="comment"># 设置时间语义</span></span><br><span class="line">env.add_jars(...) <span class="comment"># 添加第三方组件的依赖</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;故障恢复&amp;容错&quot;&quot;&quot;</span></span><br><span class="line">env.enable_checkpointing(...) <span class="comment"># 为流处理作业启用检查点</span></span><br><span class="line">env.set_restart_strategy(...) <span class="comment"># 设置重启策略</span></span><br><span class="line">env.set_state_backend(...) <span class="comment"># 设置状态后端（RocksDB并未直接包含在flink中，需要引入依赖。）</span></span><br></pre></td></tr></table></figure>



<h5 id="2-算子-1"><a href="#2-算子-1" class="headerlink" title="2.算子"></a>2.算子</h5><ul>
<li>map</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">data_stream = env.from_collection(collection=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>])</span><br><span class="line">data_stream.<span class="built_in">map</span>(<span class="keyword">lambda</span> x: <span class="number">2</span> * x, output_type=Types.INT())</span><br></pre></td></tr></table></figure>

<ul>
<li>flat_map</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">data_stream = env.from_collection(collection=[<span class="string">&#x27;hello apache flink&#x27;</span>, <span class="string">&#x27;streaming compute&#x27;</span>])</span><br><span class="line">data_stream.flat_map(<span class="keyword">lambda</span> x: x.split(<span class="string">&#x27; &#x27;</span>), result_type=Types.STRING())</span><br></pre></td></tr></table></figure>

<ul>
<li>filter</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">data_stream = env.from_collection(collection=[<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>])</span><br><span class="line">data_stream.<span class="built_in">filter</span>(<span class="keyword">lambda</span> x: x != <span class="number">0</span>)</span><br></pre></td></tr></table></figure>

<ul>
<li>key_by</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">data_stream = env.from_collection(collection=[(<span class="number">1</span>, <span class="string">&#x27;a&#x27;</span>), (<span class="number">2</span>, <span class="string">&#x27;a&#x27;</span>), (<span class="number">3</span>, <span class="string">&#x27;b&#x27;</span>)])</span><br><span class="line">data_stream.key_by(<span class="keyword">lambda</span> x: x[<span class="number">1</span>], key_type_info=Types.STRING())</span><br></pre></td></tr></table></figure>

<ul>
<li>reduce</li>
</ul>
<p>演示：rolling_aggregate.py</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">data_stream = env.from_collection(collection=[(<span class="number">1</span>, <span class="string">&#x27;a&#x27;</span>), (<span class="number">2</span>, <span class="string">&#x27;a&#x27;</span>), (<span class="number">3</span>, <span class="string">&#x27;a&#x27;</span>), (<span class="number">4</span>, <span class="string">&#x27;b&#x27;</span>)], type_info=Types.ROW([Types.INT(), Types.STRING()]))</span><br><span class="line">data_stream.key_by(<span class="keyword">lambda</span> x: x[<span class="number">1</span>]).reduce(<span class="keyword">lambda</span> a, b: (a[<span class="number">0</span>] + b[<span class="number">0</span>], b[<span class="number">1</span>]))</span><br></pre></td></tr></table></figure>

<ul>
<li><p>max, min, sum等滚动聚合函数、window算子没有在PyFlink1.12.0中有对应接口实现。</p>
</li>
<li><p>shuffle 设置DataStream分区，以随机的方式将输出元素分布到下一个操作算子的实例上。</p>
</li>
<li><p>rebalance 设置DataStream分区，以轮询的方式将输出元素均匀分布到下一个操作算子的实例上。</p>
</li>
</ul>
<h5 id="3-Source-Sink-1"><a href="#3-Source-Sink-1" class="headerlink" title="3.Source&amp;Sink"></a>3.Source&amp;Sink</h5><p>PyFlink1.12.0的datastream.connector模块下，包含了一些内建可用的Source&#x2F;Sink连接器，目前内置的连接器如下：</p>
<table>
<thead>
<tr>
<th align="center">组件</th>
<th align="center">Source</th>
<th align="center">Sink</th>
</tr>
</thead>
<tbody><tr>
<td align="center">KafkaConnector</td>
<td align="center">支持</td>
<td align="center">支持</td>
</tr>
<tr>
<td align="center">JDBCConnector</td>
<td align="center">不支持</td>
<td align="center">支持</td>
</tr>
<tr>
<td align="center">StreamingFileConnector</td>
<td align="center">不支持</td>
<td align="center">支持</td>
</tr>
</tbody></table>
<ul>
<li>FlinkKafkaConsumer</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义Kafka SourceFunction</span></span><br><span class="line">kafka_consumer = FlinkKafkaConsumer(</span><br><span class="line">    topics=<span class="string">&quot;zer0py2c_source&quot;</span>,</span><br><span class="line">    deserialization_schema=SimpleStringSchema(),</span><br><span class="line">    properties=&#123;</span><br><span class="line">        <span class="string">&quot;bootstrap.servers&quot;</span>: <span class="string">&quot;localhost:9092&quot;</span>,</span><br><span class="line">        <span class="string">&quot;zookeeper.connect&quot;</span>: <span class="string">&quot;localhost:2181&quot;</span></span><br><span class="line">    &#125;</span><br><span class="line">)</span><br><span class="line"><span class="comment"># 对接数据流</span></span><br><span class="line">env.add_source(kafka_consumer)</span><br></pre></td></tr></table></figure>

<ul>
<li>FlinkKafkaProducer</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义Kafka SinkFunction</span></span><br><span class="line">kafka_producer = FlinkKafkaProducer(</span><br><span class="line">    topic=<span class="string">&quot;zer0py2c_sink&quot;</span>,</span><br><span class="line">    serialization_schema=SimpleStringSchema(),</span><br><span class="line">    producer_config=&#123;</span><br><span class="line">        <span class="string">&quot;bootstrap.servers&quot;</span>: <span class="string">&quot;localhost:9092&quot;</span>,</span><br><span class="line">        <span class="string">&quot;zookeeper.connect&quot;</span>: <span class="string">&quot;localhost:2181&quot;</span></span><br><span class="line">    &#125;</span><br><span class="line">)</span><br><span class="line"><span class="comment"># 对接数据流</span></span><br><span class="line">data_stream.add_sink(kafka_producer)</span><br></pre></td></tr></table></figure>

<ul>
<li>JdbcSink</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 配置MySQL访问参数</span></span><br><span class="line">options = JdbcConnectionOptions.JdbcConnectionOptionsBuilder() \</span><br><span class="line">    .with_user_name(<span class="string">&quot;zer0py2c&quot;</span>) \</span><br><span class="line">    .with_password(<span class="string">&quot;123456&quot;</span>) \</span><br><span class="line">    .with_driver_name(<span class="string">&quot;com.mysql.jdbc.Driver&quot;</span>) \</span><br><span class="line">    .with_url(<span class="string">&quot;jdbc:mysql://localhost:3306/flink&quot;</span>) \</span><br><span class="line">    .build()</span><br><span class="line"><span class="comment"># 定义MySQL SinkFunciton</span></span><br><span class="line">mysql_sink = JdbcSink.sink(</span><br><span class="line">    <span class="string">&quot;insert into test_sink_table(`id`, `content`) values(?, ?)&quot;</span>,</span><br><span class="line">    type_info=Types.ROW([Types.INT(), Types.STRING()]),</span><br><span class="line">    jdbc_connection_options=options</span><br><span class="line">)</span><br><span class="line"><span class="comment"># 对接流数据</span></span><br><span class="line">array = [(<span class="number">1</span>, <span class="string">&#x27;aaa|bb&#x27;</span>), (<span class="number">2</span>, <span class="string">&#x27;bb|a&#x27;</span>), (<span class="number">3</span>, <span class="string">&#x27;aaa|a&#x27;</span>)]</span><br><span class="line">data_stream = env.from_collection(array)</span><br><span class="line">data_stream.add_sink(mysql_sink)</span><br></pre></td></tr></table></figure>

<ul>
<li>StreamingFileSink</li>
</ul>
<p>StreamingFileSink将数据写入存储桶，每个桶中的数据被组织为多个Part File，每个Part File由Sink的一个子任务负责写入。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义StreamingFile SinkFunction</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">	如下代码指定了滚动策略：只要下列三个条件中的一条成立，则滚动当前正在写入的Part File</span></span><br><span class="line"><span class="string">        Part File保持打开状态已达到60秒</span></span><br><span class="line"><span class="string">        Part File在过去60秒内都没有收到数据</span></span><br><span class="line"><span class="string">        Part File体量已经达到128MB</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line">bucket_sink = StreamingFileSink.for_row_format(</span><br><span class="line">    <span class="string">&quot;file:///tmp/bucket&quot;</span>, SimpleStringEncoder()</span><br><span class="line">).with_rolling_policy(</span><br><span class="line">    DefaultRollingPolicy.builder()</span><br><span class="line">        .with_rollover_interval(<span class="number">60</span> * <span class="number">1000</span>)</span><br><span class="line">        .with_inactivity_interval(<span class="number">60</span> * <span class="number">1000</span>)</span><br><span class="line">        .with_max_part_size(<span class="number">128</span> * <span class="number">1024</span> * <span class="number">1024</span>)</span><br><span class="line">        .build()</span><br><span class="line">).with_output_file_config(</span><br><span class="line">    OutputFileConfig.builder()</span><br><span class="line">        .with_part_prefix(<span class="string">&quot;platform_process_log&quot;</span>)</span><br><span class="line">        .with_part_suffix(<span class="string">&quot;.csv&quot;</span>)</span><br><span class="line">        .build()</span><br><span class="line">)</span><br><span class="line"><span class="comment"># 对接数据流</span></span><br><span class="line">data_stream.add_sink(bucket_sink)</span><br></pre></td></tr></table></figure>



<h3 id="四、时间语义与窗口"><a href="#四、时间语义与窗口" class="headerlink" title="四、时间语义与窗口"></a>四、时间语义与窗口</h3><h5 id="1-时间语义"><a href="#1-时间语义" class="headerlink" title="1.时间语义"></a>1.时间语义</h5><blockquote>
<p><code>EventTime</code> 数据创建时间，可以从原始数据中提取时间戳</p>
<p><code>IngestionTime</code> 数据接入Flink的时间</p>
<p><code>ProcessingTime</code> 数据进入Flink操作算子的时间</p>
</blockquote>
<p>由于网络波动等原因，Source数据通常不会按照EventTime的先后顺序进入Flink，在经过Transformation处理后输出无序也是自然的。考虑这样的场景，业务需要统计出某样本管理平台每小时内不同用户的样本下载情况（日志），如果应用默认的处理时间，那么统计出来的结果将和预期偏差较大！</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tumb_window = Tumble.over(<span class="string">&quot;60.minutes&quot;</span>).on(<span class="string">&quot;rowtime&quot;</span>).alias(<span class="string">&quot;w&quot;</span>)</span><br><span class="line">my_table.window(tumb_window) \</span><br><span class="line">		.group_by(<span class="string">&quot;username&quot;</span>, <span class="string">&quot;w&quot;</span>) \</span><br><span class="line">    	.select(<span class="string">&quot;username, count(sample_hash) as total, w.start, w.end&quot;</span>)</span><br></pre></td></tr></table></figure>



<p>因此务必做出如下两个保证：</p>
<p>（1）让Flink以EventTime语义来处理记录——Extract EventTime</p>
<p>（2）让Flink明确触发计算的条件——WaterMark</p>
<blockquote>
<p>WaterMark时间戳 &gt;&#x3D; window_end_time &amp;&amp; [window_start_time, window_end_time)内有记录</p>
</blockquote>
<ul>
<li>Table API中设置事件时间语义</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 该表应被注册为Source表</span></span><br><span class="line">t_env.connect(</span><br><span class="line">    Kafka()</span><br><span class="line">        .version(<span class="string">&quot;universal&quot;</span>)</span><br><span class="line">        .topic(<span class="string">&quot;platform_download_log&quot;</span>)</span><br><span class="line">        .<span class="built_in">property</span>(<span class="string">&quot;zookeeper.connect&quot;</span>, <span class="string">&quot;localhost:2181&quot;</span>)</span><br><span class="line">        .<span class="built_in">property</span>(<span class="string">&quot;bootstrap.servers&quot;</span>, <span class="string">&quot;localhost:9092&quot;</span>)</span><br><span class="line">    	.start_from_latest()</span><br><span class="line">).with_format(</span><br><span class="line">    Json()</span><br><span class="line">        .fail_on_missing_field(<span class="literal">False</span>)</span><br><span class="line">        .schema(DataTypes.ROW([</span><br><span class="line">            DataTypes.FIELD(<span class="string">&quot;username&quot;</span>, DataTypes.STRING()),</span><br><span class="line">            DataTypes.FIELD(<span class="string">&quot;sample_hash&quot;</span>, DataTypes.STRING()),</span><br><span class="line">            DataTypes.FIELD(<span class="string">&quot;api_or_web&quot;</span>, DataTypes.TINYINT()),</span><br><span class="line">            DataTypes.FIELD(<span class="string">&quot;download_time&quot;</span>, DataTypes.TIMESTAMP(<span class="number">3</span>)),</span><br><span class="line">        ]))</span><br><span class="line">).with_schema(</span><br><span class="line">    Schema()</span><br><span class="line">        .field(<span class="string">&quot;username&quot;</span>, DataTypes.STRING())</span><br><span class="line">        .field(<span class="string">&quot;sample_hash&quot;</span>, DataTypes.STRING())</span><br><span class="line">        .field(<span class="string">&quot;api_or_web&quot;</span>, DataTypes.TINYINT())</span><br><span class="line">        .field(<span class="string">&quot;download_time&quot;</span>, DataTypes.TIMESTAMP(<span class="number">3</span>))</span><br><span class="line">        .rowtime(</span><br><span class="line">            <span class="comment"># watermark时间戳 = 当前最大时间戳 - 指定delay</span></span><br><span class="line">            Rowtime()</span><br><span class="line">                .timestamps_from_field(<span class="string">&quot;download_time&quot;</span>) <span class="comment"># 从字段中提取时间戳作为EventTime</span></span><br><span class="line">                .watermarks_periodic_bounded(<span class="number">60</span> * <span class="number">1000</span>) <span class="comment"># 周期性生成watermark</span></span><br><span class="line">        )</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<ul>
<li>SQL中定义事件时间语义</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 该表应该被注册为Source表</span></span><br><span class="line">t_env.sql_update(</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">	create table KafkaSource (</span></span><br><span class="line"><span class="string">		username varchar,</span></span><br><span class="line"><span class="string">		sample_hash varchar,</span></span><br><span class="line"><span class="string">		api_or_web tinyint,</span></span><br><span class="line"><span class="string">		download_time timestamp(3),</span></span><br><span class="line"><span class="string">		watermark for download_time as download_time - interval &#x27;60&#x27; second # 设置EventTime</span></span><br><span class="line"><span class="string">	) with (</span></span><br><span class="line"><span class="string">		&#x27;connector.type&#x27; = &#x27;kafka&#x27;,</span></span><br><span class="line"><span class="string">		&#x27;connector.version&#x27; = &#x27;universal&#x27;, # 不必指定具体的版本号</span></span><br><span class="line"><span class="string">		&#x27;connector.topic&#x27; = &#x27;platform_download_log&#x27;,</span></span><br><span class="line"><span class="string">		&#x27;connector.properties.zookeeper.connect&#x27; = &#x27;localhost:2181&#x27;,</span></span><br><span class="line"><span class="string">		&#x27;connector.properties.bootstrap.servers&#x27; = &#x27;localhost:9092&#x27;,</span></span><br><span class="line"><span class="string">		&#x27;connector.properties.scan.startup.mode&#x27; = &#x27;latest-offset&#x27;,</span></span><br><span class="line"><span class="string">		&#x27;format.type&#x27; = &#x27;csv&#x27;,</span></span><br><span class="line"><span class="string">		&#x27;format.ignore-parse-errors&#x27; = &#x27;true&#x27; # 解析异常时，跳过当前字段数据（字段将置为null）</span></span><br><span class="line"><span class="string">	)</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>



<h5 id="2-窗口类型"><a href="#2-窗口类型" class="headerlink" title="2.窗口类型"></a>2.窗口类型</h5><ul>
<li>滚动窗口Tumbling Window</li>
</ul>
<p>将数据按照固定的窗口长度进行划分，窗口无重叠。这个固定的窗口长度既可以是<code>时间</code>， 也可以是<code>数量</code>。</p>
<p><img src="/images/tumbling-windows.svg" alt="tumbling-windows"></p>
<ul>
<li>滑动窗口Sliding Window</li>
</ul>
<p>将数据按照固定的窗口长度+滑动间隔进行划分，窗口有重叠。这个固定的窗口长度既可以是<code>时间</code>，也可以是<code>数量</code>。当“滑动间隔&#x3D;窗口长度”时，退化为滚动窗口。</p>
<p><img src="/images/sliding-windows.svg" alt="sliding-windows"></p>
<ul>
<li>会话窗口Session Window</li>
</ul>
<p>指定一个具有时间长度的Gap，若Gap时间内都没有收到新数据，则开一个新的窗口。</p>
<p><img src="/images/session-windows.svg" alt="session-windows"></p>
<h5 id="3-Group-Window"><a href="#3-Group-Window" class="headerlink" title="3.Group Window"></a>3.Group Window</h5><p>时间语义，需要结合窗口操作才能充分应用起来。如下以Table API为例，介绍Flink中窗口的代码操作。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">my_table = t_env.from_path(...)</span><br><span class="line">my_table.window(my_window <span class="keyword">as</span> <span class="string">&quot;w&quot;</span>)</span><br><span class="line">		.group_by(<span class="string">&quot;other_column&quot;</span>, <span class="string">&quot;w&quot;</span>)</span><br><span class="line">    	.select(<span class="string">&quot;...&quot;</span>)</span><br></pre></td></tr></table></figure>

<ul>
<li>滚动窗口</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Tumbling EventTime Window</span></span><br><span class="line">tumb_window = Tumble.over(<span class="string">&quot;10.minutes&quot;</span>).on(<span class="string">&quot;rowtime&quot;</span>).alias(<span class="string">&quot;w&quot;</span>)</span><br><span class="line"><span class="comment"># Tumbling ProcessingTime Window</span></span><br><span class="line">tumb_window = Tumble.over(<span class="string">&quot;10.minutes&quot;</span>).on(<span class="string">&quot;proctime&quot;</span>).alias(<span class="string">&quot;w&quot;</span>)</span><br><span class="line"><span class="comment"># Tumbling RowCount Window</span></span><br><span class="line">tumb_window = Tumble.over(<span class="string">&quot;10.rows&quot;</span>).on(<span class="string">&quot;proctime&quot;</span>).alias(<span class="string">&quot;w&quot;</span>)</span><br></pre></td></tr></table></figure>

<ul>
<li>滑动窗口</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Sliding EventTime Window</span></span><br><span class="line">slid_window = Slide.over(<span class="string">&quot;10.minutes&quot;</span>).every(<span class="string">&quot;5.minutes&quot;</span>).on(<span class="string">&quot;rowtime&quot;</span>).alias(<span class="string">&quot;w&quot;</span>)</span><br><span class="line"><span class="comment"># Sliding ProcessingTime Window</span></span><br><span class="line">slid_window = Slide.over(<span class="string">&quot;10.minutes&quot;</span>).every(<span class="string">&quot;5.minutes&quot;</span>).on(<span class="string">&quot;proctime&quot;</span>).alias(<span class="string">&quot;w&quot;</span>)</span><br><span class="line"><span class="comment"># Sliding RowCount Window</span></span><br><span class="line">slid_window = Slide.over(<span class="string">&quot;10.rows&quot;</span>).every(<span class="string">&quot;5.minutes&quot;</span>).on(<span class="string">&quot;proctime&quot;</span>).alias(<span class="string">&quot;w&quot;</span>)</span><br></pre></td></tr></table></figure>

<ul>
<li>会话窗口</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Session EventTime Window</span></span><br><span class="line">session_window = Session.with_gap(<span class="string">&quot;10.minutes&quot;</span>).on(<span class="string">&quot;rowtime&quot;</span>).alias(<span class="string">&quot;w&quot;</span>)</span><br><span class="line"><span class="comment"># Session ProcessingTime Window</span></span><br><span class="line">session_window = Session.with_gap(<span class="string">&quot;10.minutes&quot;</span>).on(<span class="string">&quot;proctime&quot;</span>).alias(<span class="string">&quot;w&quot;</span>)</span><br></pre></td></tr></table></figure>



<h3 id="五、容错配置"><a href="#五、容错配置" class="headerlink" title="五、容错配置"></a>五、容错配置</h3><h5 id="1-启用一致性checkpoint"><a href="#1-启用一致性checkpoint" class="headerlink" title="1.启用一致性checkpoint"></a>1.启用一致性checkpoint</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 为流式处理作业启用检查点。将定期对数据流的分布式状态进行快照。如果发生故障，将从最近完成的检查点重新启动。</span></span><br><span class="line">env.enable_checkpointing(mode=CheckpointingMode.EXACTLY_ONCE) </span><br><span class="line"><span class="comment"># 每300ms进行一次checkpoint</span></span><br><span class="line">env.get_checkpoint_config().set_checkpoint_interval(<span class="number">300</span>)</span><br><span class="line"><span class="comment"># 设置一次进行checkpoint的超时时间</span></span><br><span class="line">env.get_checkpoint_config().set_checkpoint_timeout(<span class="number">60000</span>)</span><br><span class="line"><span class="comment"># 等当前快照生成之后，才能进行下一个快照</span></span><br><span class="line">env.get_checkpoint_config().set_max_concurrent_checkpoints(<span class="number">1</span>)</span><br><span class="line"><span class="comment"># 前一次checkpoint保存结束到下一次checkpoint触发开始之间的最小时间间隔</span></span><br><span class="line">env.get_checkpoint_config().set_min_pause_between_checkpoints(<span class="number">100</span>)</span><br><span class="line"><span class="comment"># checkpoint失败，则任务执行失败</span></span><br><span class="line">env.get_checkpoint_config().set_fail_on_checkpointing_errors(<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>



<h5 id="2-配置状态后端statebackend"><a href="#2-配置状态后端statebackend" class="headerlink" title="2.配置状态后端statebackend"></a>2.配置状态后端statebackend</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># state保存在taskmanager中，checkpoint的状态快照保存在jobmanager中（默认）</span></span><br><span class="line">env.set_state_backend(MemoryStateBackend())</span><br><span class="line"><span class="comment"># state保存在taskmanager中，checkpoint的状态快照保存在指定好的文件系统中</span></span><br><span class="line">env.set_state_backend(FsStateBackend(<span class="string">&quot;file:///var/checkpoints/&quot;</span>))</span><br><span class="line"><span class="comment"># state保存在RocksDB中，checkpoint的状态快照保存在指定好的文件系统中</span></span><br><span class="line">env.add_jar(<span class="string">&quot;file:///.../flink-statebackend-rocksdb_2.11-1.12.0.jar&quot;</span>)</span><br><span class="line">env.set_state_backend(RocksDBStateBackend(<span class="string">&quot;file:///var/checkpoints/&quot;</span>))</span><br></pre></td></tr></table></figure>



<h5 id="3-配置重启策略"><a href="#3-配置重启策略" class="headerlink" title="3.配置重启策略"></a>3.配置重启策略</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 默认失败后不尝试重启</span></span><br><span class="line">env.set_restart_strategy(RestartStrategies.no_restart())</span><br><span class="line"><span class="comment"># 固定延迟策略（Job执行发生故障，系统会尝试重启3次Job，10秒后尝试下一次重启）</span></span><br><span class="line">env.set_restart_strategy(RestartStrategies.fixed_delay_restart(<span class="number">3</span>, <span class="number">10</span> * <span class="number">1000</span>))</span><br><span class="line"><span class="comment"># 故障率策略（如果5分钟内失败了3次，则Job执行失败，重试间隔为10秒）</span></span><br><span class="line">env.set_restart_strategy(RestartStrategies.failure_rate_restart(<span class="number">3</span>, <span class="number">5</span> * <span class="number">60</span> * <span class="number">1000</span>, <span class="number">10</span> * <span class="number">1000</span>))</span><br></pre></td></tr></table></figure>



<h3 id="六、一个案例"><a href="#六、一个案例" class="headerlink" title="六、一个案例"></a>六、一个案例</h3><p><a target="_blank" rel="noopener" href="https://my.oschina.net/u/2828172/blog/4387503">基于PyFlink流计算的商品价格实时监控系统</a></p>

					</div>

			  <!-- about -->
			  
		</div>

		<!-- pagination -->
	  

		<div class="comment-section">
  
  


</div>
	</div>

	

</div>


		<footer>
			

<p>
  由 <a target="_blank" rel="noopener" href="https://hexo.io">hexo</a> 强力驱动 | 搭载 <a target="_blank" rel="noopener" href="https://github.com/wayou/hexo-theme-material">material</a> 主题
</p>
<p>
  &copy; 2024 <a href="http://example.com"> zer0py2c </a>
</p>
<a id="gotop" href="#" title="back to top"><i class="mdi-hardware-keyboard-arrow-up"></i></a>

		</footer>
	  </div>

		<!-- <script src="/libs/bs/js/bootstrap.min.js"></script> -->
		<script src="//apps.bdimg.com/libs/bootstrap/3.3.4/js/bootstrap.min.js"></script>
		<script>(typeof $().modal == 'function')|| document.write('<script src="/libs/bs/js/bootstrap.min.js" type="text/javascript"><\/script>')</script>

		<!-- material design -->
		<!-- <script src="/libs/bs-material/js/ripples.min.js"></script> -->
		<script src="//apps.bdimg.com/libs/bootstrap-material/0.3.0/js/ripples.min.js"></script>
		<!-- <script src="/libs/bs-material/js/material.min.js"></script> -->
		<script src="//apps.bdimg.com/libs/bootstrap-material/0.3.0/js/material.min.js"></script>
		<!-- toc -->
		<!-- <script src="/libs/tocify/jquery-ui.min.js"></script> -->
		<script src="//apps.bdimg.com/libs/jqueryui/1.10.4/jquery-ui.min.js"></script>
		<script src="/libs/tocify/jquery.tocify.custom.js"></script>

		<script src="/js/main.js"></script>

	</body>
</html>
